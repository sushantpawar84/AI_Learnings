Concepts of LLM and AI: 
NLP -> ML -> LLM -> Gen AI

Some Python framework and libraries:
1. Torch framework
2. Transformers lib
3. Datasets lib
4. Accelerate lib
5. peft - Parameter-Efficient-Fine-Tunning Lib

Pre Training and Fine Tunning. 
First stage of LLM is called Pretraining. Ex. GPT3 model is a pretrained model which is capable of text completion
After pretraining, we can further train LLM which is called FINETUNNING. 
FINETUNNING are of 2 categories:
Intruction Finetunning
Finetunning for Classification Tasks
Finetunning happens on labelled data. Pre Training happens on raw data. 

======================
Transformers:
======================
Transformer Achitechture = Deep neural network architecture introduced in 2017
Steps of Transformer Architecture:
1. Input Text
2. Tokenization of the text
3. Encoder - Vector Embedding of those Tokens
4. Embedding
5. Partial Text
6. Preprocessing Steps - I/p text is prepared for decoder
7. Decoder - Generates translated - one word at a time
8. Output Text with full result.

Architecture is mainly have 2 block:
1. Encoder - Encodes i/p  text  into  vectors
2. Decoder -  Generates o/p text from encoded vectors.

Self Attention Mechanism: Key part of transformer. Allows model to weigh importance of different words/tokens relative to each other.
Enable models to capture long range dependencies.
BERT = Bidirectional Encoder Representation from Transformers - This doesn't have Decoder, Only Encoder
GPT = Generative pretrained Transformer - This doesn't have Encoder, only have Decoder

====================
Transformer vs LLM
====================
- Not all transformers are LLMs
- Transformers can be used for other tasks like Computer Vision
- Not all LLMs are Transformers
- LLMs can be based on recurrent or convolutional architectures as well
- LLMs can be used to build Long Short Term Memory Networks for Analytical tasks

======================
Closer Look at GPT
======================
Zero Shot - Ability to generalize to completely unseen task without any prior specific examples. Without any CONTEXT
Few Shot -Learning from a minimum number of examples which the user provides as input. With CONTEXT



